{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Indexify","text":"<p>Indexify is a knowledge and memory retrieval service for Large Language Models. It faciliates in-context learning of LLMs by providing relevant context in a prompt or expsing relevant memory to AI agents.</p> <p>The service facilitates efficient execution of fine tuned/pre-trained embedding models and expose them over APIs. Several state of the art retreival algorithms are implemented to provide a batteries-included retrieval experience.</p>"},{"location":"#why-use-indexify","title":"Why use Indexify","text":"<ul> <li>Flexibility: An API based embedding serving and index querying approach allows easy integrations without needing native libraries for every language.</li> <li>Reduced Footprint: Models and inference runtime like PyTorch are large, Indexify alleviates the need to package them with applications.</li> <li>Scalability: Indexify provides hardware optimized versions of the models whenever possible.</li> <li>State of the Art Embedding Models - As new embedding models are developed we will add support for them in the service, without applications needing to be updated or re-packaged.</li> <li>Integration with Langchain, Deepset and NextJS - Integration via indexify python and TypeScript libraries.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Download the Indexify Container </li> <li>Update the server configuration to turn on more models than the defaults.</li> <li>Consume Embedding and Index APIs from applications.</li> </ol>"},{"location":"#available-embedding-models","title":"Available Embedding Models","text":"<ol> <li>all-MiniLM-L12-v2</li> <li>sentence-t5-base</li> <li>SimCSE (coming soon)</li> <li>OpenAI</li> </ol>"},{"location":"#available-vector-datastores","title":"Available Vector Datastores","text":"<ol> <li>Qdrant</li> <li>Pinecone(Coming Soon)</li> <li>Milvus(Coming Soon)</li> <li>Ephemeral Storage(Coming Soon).</li> </ol>"},{"location":"#http-apis","title":"HTTP APIs","text":""},{"location":"#list-embedding-models-available","title":"List Embedding Models Available","text":"<pre><code>GET /embedding/list-models\n</code></pre>"},{"location":"#generate-embeddings","title":"Generate Embeddings","text":"<pre><code>GET /embedding/create\n</code></pre>"},{"location":"#request-body","title":"Request Body","text":"<p>model string ID of the model to use. You can use the List models API to see all of your available models.</p> <p>inputs array of text Array of texts for which embeddings have to be generated</p>"},{"location":"#query-index-by-text","title":"Query Index By Text","text":"<pre><code>GET /index?text=&lt;text&gt;&amp;algorithm=&lt;algorithm&gt;&amp;limit=&lt;top-k&gt;\n</code></pre>"},{"location":"#generate-embeddings-and-write-to-index","title":"Generate Embeddings and Write to Index","text":"<pre><code>POST /index/write\n</code></pre>"}]}